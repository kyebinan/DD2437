# Deep Learning
My Artificial Neural Network laboratories from my KTH's course

# I-Multi-layer perceptrons: learning and generalization
  ### Laboratory 1A: Single-layer perceptron
  This lab assignment focuses on learning approaches for a single-layer perceptron, also known as a threshold logic unit or McCulloch-Pitts neuron, in simple binary classification problems. The two main learning algorithms explored are the classical perceptron and the delta (Widrow-Hoff) rule. The objectives include designing and applying perceptrons for classification tasks, identifying limitations of single-layer networks and their learning methods, and configuring and monitoring the behavior of learning algorithms. The lab's scope involves implementing perceptrons focusing on the classical perceptron and Widrow-Hoff delta rule algorithms. 

  ### Laboratory 1B: Multilayer Perceptron (MLP) 
  This lab assignment focuses on Multilayer Perceptron (MLP) networks, emphasizing supervised learning algorithms. Participants will implement MLP networks and study their properties through simulations. The assignment is divided into two parts: the first involves developing code from scratch, focusing on the generalized delta rule for a two-layer perceptron (also known as backpropagation), applied to tasks such as classification, data compression, and function approximation. The second part addresses chaotic time series prediction using MLPs, where we will design, train, validate, and evaluate neural networks to achieve robust solutions with good generalization capabilities.
  
# II-Competitive learning, self-organizing maps
  ### Laboratory 2: Competitive learning (CL) and self-organizing concepts
  This lab assignment focuses on unsupervised neural network approaches, specifically competitive learning (CL) and self-organizing concepts. We will experiment with Radial-Basis Function (RBF) networks, combining unsupervised and supervised learning for classification and regression tasks. The second part delves into self-organizing neural networks, particularly Kohonen maps or Self-Organizing Maps (SOMs). SOMs map high-dimensional input space points to a low-dimensional output space, aiding in visualizing and clustering complex data. The objectives include building and training RBF networks, analyzing methods for initializing structures and learning weights, understanding vector quantization, implementing SOM components, discussing the neighborhood's role, and utilizing SOMs to fold high-dimensional spaces and cluster data. The first part involves RBF networks for function approximation, employing a CL algorithm for unit initialization. In the second part, we implement the core SOM algorithm for tasks like ordering objects based on attributes, finding a circular tour through prescribed points, and mapping voting behavior in the Swedish parliament to achieve low-dimensional representations of higher-dimensional data.

# III-Recurrent architectures and stochastic networks
  ### Laboratory 3: Hopfield networks and associative memory
  This exercise centers around the exploration of Hopfield networks and associative memory, aiming at the ability to comprehend and apply principles related to auto-associative networks. The focus is on constructing Hopfield-type auto-associative memories using the Hebbian learning principle and investigating their capabilities, capacity, and limitations. Recurrent neural networks, particularly Hopfield networks, are introduced, showcasing their significance in associative memory applications. The Hopfield network, a fully connected auto-associative memory network with two-state neurons, is extensively studied due to its analyzability using statistical mechanics methods. The assignment delves into the discrete version of the Hopfield network, employing the Hebbian learning rule and exploring both synchronous and asynchronous update mechanisms during recall. We are guided through tasks involving training the Hopfield network, understanding attractor dynamics, pattern completion, noise reduction, and investigating storage capacity.

# IV-Deep neural networks: principles and implementations
  ### Laboratory 4: Restricted Boltzmann Machines and Deep Belief Nets
  The primary objective of this lab is to provide us with a hands-on experience and understanding of key components in deep neural network (DNN) architectures, specifically focusing on Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs). Upon completion of the assignment, participants should be proficient in explaining the fundamental concepts underlying RBM learning processes, employing algorithms for unsupervised pretraining of RBM layers, and supervised fine-tuning of resulting DBNs. The lab encourages participants to design multi-layer neural network architectures for classification problems based on RBM layers and explore the generative aspects of DBNs. The lab provides a comprehensive exploration of RBMs and DBNs, fostering practical skills in neural network architecture design and functionality analysis.
  
